{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0b8a944a",
      "metadata": {
        "id": "0b8a944a"
      },
      "source": [
        "## Project: ViT vs CNN"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "JR6I7my9CF9H",
      "metadata": {
        "id": "JR6I7my9CF9H"
      },
      "source": [
        "The objective of this project is to compare the performances of two different model in the task of image classification: Vision Transformers (ViT) an Convolutional Neural Network (CNN). In this project, we will use three different labelled datasets for comparision: CIFAR-10 (10 classes), CIFAR-100 (100 classes) and Imagenet-200 (200 classes)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "99bb45f7",
      "metadata": {
        "id": "99bb45f7"
      },
      "source": [
        "This project use information from these sources:\n",
        "\n",
        "_ An Image is Worth 16x16 Words Transformers for Image Recognition at Scale, Vision Transformer, ViT, by Google Research, Brain Team 2021 ICLR. https://arxiv.org/abs/2010.11929\n",
        "\n",
        "_ https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/\n",
        "\n",
        "_ https://sh-tsang.medium.com/review-vision-transformer-vit-406568603de0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "laowmO1QMma5",
      "metadata": {
        "id": "laowmO1QMma5"
      },
      "source": [
        "##### **1. Prepare the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1fb02427",
      "metadata": {
        "id": "1fb02427",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Import necessary library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import kagglehub\n",
        "import shutil\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "uRZgI6cUZXUi",
      "metadata": {
        "id": "uRZgI6cUZXUi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def prepare_writable_data(read_only_dir):\n",
        "    \"\"\"Copies the dataset from the read-only input path to the writable working path.\"\"\"\n",
        "\n",
        "    if read_only_dir is None or not os.path.exists(read_only_dir):\n",
        "        return None\n",
        "\n",
        "    # Define the destination path in the writable working directory\n",
        "    writable_dir = os.path.join('/kaggle/working/', 'tiny-imagenet-writable')\n",
        "\n",
        "    if os.path.exists(writable_dir):\n",
        "        print(\"Writable directory already exists. Skipping copy.\")\n",
        "        return writable_dir\n",
        "\n",
        "    print(f\"Copying data from {read_only_dir} to {writable_dir}...\")\n",
        "\n",
        "    try:\n",
        "        # Use copytree to copy the entire directory structure\n",
        "        shutil.copytree(read_only_dir, writable_dir)\n",
        "        print(\"Data copied successfully.\")\n",
        "        return writable_dir\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data copying: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8f1072b6",
      "metadata": {
        "id": "8f1072b6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def sort_tiny_imagenet_validation(data_dir):\n",
        "    \"\"\"\n",
        "    Sorts the validation images of Tiny ImageNet into class-specific folders.\n",
        "    This is necessary because the raw dataset dumps all validation images\n",
        "    into a single folder, while ImageFolder expects class subdirectories.\n",
        "    \"\"\"\n",
        "\n",
        "    val_dir = os.path.join(data_dir, 'val')\n",
        "    val_images_dir = os.path.join(val_dir, 'images')\n",
        "    annotations_file = os.path.join(val_dir, 'val_annotations.txt')\n",
        "\n",
        "    print(\"Sorting Tiny ImageNet validation set...\")\n",
        "\n",
        "    # 1. Read annotations and create class directories\n",
        "    with open(annotations_file, 'r') as f:\n",
        "        annotations = f.readlines()\n",
        "\n",
        "    # 2. Process each annotation line\n",
        "    for line in tqdm(annotations, desc=\"Processing validation images\"):\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        filename = parts[0]  # e.g., 'val_0.JPEG'\n",
        "        synset_id = parts[1] # e.g., 'n01440764'\n",
        "\n",
        "        # Define source and destination paths\n",
        "        src_path = os.path.join(val_images_dir, filename)\n",
        "        dst_dir = os.path.join(val_dir, synset_id)\n",
        "        dst_path = os.path.join(dst_dir, filename)\n",
        "\n",
        "        # Create the destination class directory if it doesn't exist\n",
        "        os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "        # Move the image\n",
        "        if os.path.exists(src_path):\n",
        "            shutil.move(src_path, dst_path)\n",
        "\n",
        "    # 3. Clean up the original 'images' folder and annotations file\n",
        "    if os.path.exists(val_images_dir):\n",
        "        try:\n",
        "            os.rmdir(val_images_dir)\n",
        "        except OSError:\n",
        "            # Directory might not be empty if some files failed to move\n",
        "            print(\"Warning: Could not remove original 'val/images' directory.\")\n",
        "\n",
        "    print(\"Validation set sorting complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "853e1ea3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "853e1ea3",
        "outputId": "0ca65cbb-8526-4f21-bf03-bcd6050abca4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to download Tiny ImageNet via KaggleHub...\n",
            "Using Colab cache for faster access to the 'tinyimagenet200' dataset.\n",
            "Tiny ImageNet downloaded successfully to: /kaggle/input/tinyimagenet200\n"
          ]
        }
      ],
      "source": [
        "print(\"Attempting to download Tiny ImageNet via KaggleHub...\")\n",
        "\n",
        "# This returns the local path where the dataset files are stored\n",
        "TINY_IMAGENET_PATH = kagglehub.dataset_download(\"nikhilshingadiya/tinyimagenet200\")\n",
        "TINY_IMAGENET_ROOT = None\n",
        "WRITABLE_DATA_ROOT = None \n",
        "print(f\"Tiny ImageNet downloaded successfully to: {TINY_IMAGENET_PATH}\")\n",
        "\n",
        "TINY_IMAGENET_ROOT = os.path.join(TINY_IMAGENET_PATH, 'tiny-imagenet-200')\n",
        "if not os.path.isdir(TINY_IMAGENET_ROOT):\n",
        "    # If the structure is flat, the path itself might be the root\n",
        "    TINY_IMAGENET_ROOT = TINY_IMAGENET_PATH"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fa9b13ef",
      "metadata": {
        "id": "fa9b13ef"
      },
      "source": [
        "#### **2. Model architecture:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ebcd27c0",
      "metadata": {
        "id": "ebcd27c0"
      },
      "source": [
        "##### **2.1 Vision Transformer (ViT) Architecture Overview**\n",
        "\n",
        "![Vision Transformer Architecture](ViTarchi.png)\n",
        "\n",
        "The ViT treats an image as a sequence of tokens, similar to words in a sentence. It relies entirely on the Self-Attention mechanism to capture relationships between distant parts of the image (patches). It lacks the inherent inductive biases of CNNs (like locality and translation invariance), making it highly flexible but requiring large amounts of data to learn spatial relationships from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "420f1402",
      "metadata": {
        "id": "420f1402",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Patch Embedding (Tokenization):\n",
        "# This modules is to convert the original 2D images into\n",
        "# 1D sequence of embeded vectors.\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        # Conv2d implements the non-overlapping patch embedding/linear projection\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W) -> (B, D, H', W')\n",
        "        x = self.proj(x)\n",
        "        # Flatten H' x W' into sequence length N, transpose to (B, N, D)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "158f5430",
      "metadata": {
        "id": "158f5430",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# --- Vision Transformer (ViT) Model ---\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "                 embed_dim=128, depth=6, num_heads=8, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=0.1)\n",
        "\n",
        "        # Define a single standard Transformer Encoder Layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "            dropout=0.1,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        # Stack the layers using nn.TransformerEncoder\n",
        "        self.blocks = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = self.pos_drop(x + self.pos_embed)\n",
        "\n",
        "        # The standard PyTorch TransformerEncoder handles the sequence of blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Classification uses the CLS token output\n",
        "        cls_output = x[:, 0]\n",
        "        x = self.head(cls_output)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "TmZB5L8lm9eb",
      "metadata": {
        "id": "TmZB5L8lm9eb"
      },
      "source": [
        "##### **2.2 CNN architecture**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Az15m3jnnCgp",
      "metadata": {
        "id": "Az15m3jnnCgp"
      },
      "source": [
        "In this project, we will build a CNN architecture inspired by ResNet, which is highly effective for image classification.\n",
        "\n",
        "This model uses $3 \\times 3$ convolutions and gradually increases the channel depth while reducing the spatial resolution via stride-2 convolutions in the main blocks.\n",
        "\n",
        "###### **2.2.1 Structure of the Core Building Block (`CNNBlock`)**\n",
        "\n",
        "The `CNNBlock` implements the Basic Block structure from ResNet, ensuring efficient training even with limited depth.\n",
        "\n",
        "$$\n",
        "\\text{Output} = \\text{ReLU}(\\text{BN}(\\text{Conv}_2(\\text{ReLU}(\\text{BN}(\\text{Conv}_1(\\text{Input})))) + \\text{Shortcut}(\\text{Input}))\n",
        "$$\n",
        "\n",
        "| Step | Operation | Output Size |\n",
        "| :--- | :--- | :--- |\n",
        "| **Input** | Feature Map | $H \\times W \\times C_{in}$ |\n",
        "| **Conv 1** | Conv $3 \\times 3$, Stride $S$, Padding 1 | $H/S \\times W/S \\times C_{out}$ |\n",
        "| **Conv 2** | Conv $3 \\times 3$, Stride 1, Padding 1 | $H/S \\times W/S \\times C_{out}$ |\n",
        "| **Shortcut** | Conv $1 \\times 1$ (if stride $\\neq 1$ or $C_{in} \\neq C_{out}$) | $H/S \\times W/S \\times C_{out}$ |\n",
        "| **Output** | Output Feature Map (after addition and ReLU) | $H/S \\times W/S \\times C_{out}$ |\n",
        "\n",
        "---\n",
        "\n",
        "###### **2.2.2 Full Model Architecture (`CustomCNN`)**\n",
        "\n",
        "Assuming an input image size of **$32 \\times 32$** (for CIFAR), here is the layer breakdown:\n",
        "\n",
        "| Layer Name | Module/Operation | Stride | Output Channels (Depth) | Output Size (H x W) | Notes |\n",
        "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
        "| **Input** | Image | - | 3 | $32 \\times 32$ | |\n",
        "| **Conv 1** | Conv $3 \\times 3$ + BN + ReLU | 1 | 16 | $32 \\times 32$ | Initial Feature Map |\n",
        "| **Layer 1** | **2x** `CNNBlock` | 1 | 16 | $32 \\times 32$ | No spatial reduction |\n",
        "| **Layer 2** | **2x** `CNNBlock` | 2 (in first block) | 32 | $16 \\times 16$ | Spatial Downsampling (32 $\\to$ 16) |\n",
        "| **Layer 3** | **2x** `CNNBlock` | 2 (in first block) | 64 | $8 \\times 8$ | Spatial Downsampling (16 $\\to$ 8) |\n",
        "| **Avg Pool** | AdaptiveAvgPool2d | - | 64 | $1 \\times 1$ | Global Pooling |\n",
        "| **Linear** | Fully Connected | - | Num Classes | 1 | Final Classification |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "501de026",
      "metadata": {
        "id": "501de026",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        # 3x3 Conv, BN, ReLU\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        # 3x3 Conv, BN (Residual part)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut connection for ResNet-like structure\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        result = self.bn2(self.conv2(result))\n",
        "        result += self.shortcut(x)\n",
        "        result = nn.ReLU()(result)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a4c732ae",
      "metadata": {
        "id": "a4c732ae",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# --- Custom CNN Model ---\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_channels = 16\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # ResNet-like blocks (16 -> 32 -> 64 channels)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(CNNBlock(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a133fe25",
      "metadata": {
        "id": "a133fe25"
      },
      "source": [
        "#### 2 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "97fa7518",
      "metadata": {
        "id": "97fa7518",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def get_cifar_loaders(dataset_name, batch_size=64):\n",
        "    \"\"\"Downloads and prepares data loaders for CIFAR-10 or CIFAR-100.\"\"\"\n",
        "    if dataset_name == 'CIFAR-10':\n",
        "        dataset_class = datasets.CIFAR10\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'CIFAR-100':\n",
        "        dataset_class = datasets.CIFAR100\n",
        "        num_classes = 100\n",
        "\n",
        "\n",
        "    # Standard CIFAR normalization\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    train_data = dataset_class(root='./data', train=True, download=True, transform=transform)\n",
        "    test_data = dataset_class(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f\"Loaded {dataset_name}: {len(train_data)} training images, {len(test_data)} test images.\")\n",
        "    return train_loader, test_loader, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a2Zdgr8O74YL",
      "metadata": {
        "id": "a2Zdgr8O74YL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFile\n",
        "\n",
        "# Crucial setting for handling truncated images (common in ImageNet)\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def is_valid_image_file(path):\n",
        "    \"\"\"Checks if a file is a valid, non-corrupted image.\"\"\"\n",
        "    try:\n",
        "        # 1. Check if file is empty\n",
        "        if os.path.getsize(path) == 0:\n",
        "            return False\n",
        "\n",
        "        # 2. Attempt to open and verify the image header\n",
        "        img = Image.open(path)\n",
        "        img.verify() # Verify the file integrity\n",
        "        return True\n",
        "    except Exception:\n",
        "        # If PIL throws any error (UnidentifiedImageError, IOError, etc.), it's invalid\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e09fa825",
      "metadata": {
        "id": "e09fa825",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_imagenet_200_loaders(data_dir, batch_size=64):\n",
        "    \"\"\"\n",
        "    Loads ImageNet-200 (Tiny ImageNet) data.\n",
        "    \"\"\"\n",
        "    if data_dir is None:\n",
        "        print(\"Data directory is invalid. Cannot load data.\")\n",
        "        return None, None, 200\n",
        "\n",
        "    # Sort Validation Data\n",
        "    sort_tiny_imagenet_validation(data_dir)\n",
        "\n",
        "    # Standard normalization, resizing/cropping to 64x64\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(64),\n",
        "        transforms.CenterCrop(64),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        train_data = datasets.ImageFolder(\n",
        "            root=os.path.join(data_dir, 'train'),\n",
        "            transform=transform,\n",
        "            is_valid_file=is_valid_image_file # Filter out bad files\n",
        "        )\n",
        "        test_data = datasets.ImageFolder(\n",
        "            root=os.path.join(data_dir, 'val'),\n",
        "            transform=transform,\n",
        "            is_valid_file=is_valid_image_file # Filter out bad files\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading ImageNet-200 structure: {e}\")\n",
        "        return None, None, 200\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    num_classes = 200\n",
        "    print(f\"Loaded ImageNet-200: {len(train_data)} training images, {len(test_data)} test images.\")\n",
        "    return train_loader, test_loader, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "393e82ba",
      "metadata": {
        "id": "393e82ba",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, device, epochs):\n",
        "    \"\"\"Trains the model and measures total training time.\"\"\"\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        # Use tqdm for progress bar if installed\n",
        "        for i, (inputs, labels) in enumerate(train_loader): # wrap with tqdm(train_loader) if using tqdm\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    train_time = end_time - start_time\n",
        "    return train_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c8240fa7",
      "metadata": {
        "id": "c8240fa7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_model(model, test_loader, device, img_size):\n",
        "    \"\"\"Evaluates the model, calculating accuracy and average inference time.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time_total = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass (Inference)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Accuracy calculation\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    end_time_total = time.time()\n",
        "\n",
        "\n",
        "    total_inference_time_s = end_time_total - start_time_total\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate average time per image based on the total time and total images\n",
        "    avg_inference_time_ms = (total_inference_time_s / total) * 1000\n",
        "\n",
        "    print(f\"Total images tested: {total}\")\n",
        "    print(f\"Total inference time for dataset: {total_inference_time_s:.2f} seconds\")\n",
        "\n",
        "    return accuracy, avg_inference_time_ms\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Returns the total number of trainable parameters in the model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0da4b78e",
      "metadata": {
        "id": "0da4b78e"
      },
      "source": [
        "#### **3. Experiments with different datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "IXJVHlNkLpQD",
      "metadata": {
        "id": "IXJVHlNkLpQD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize results list\n",
        "FINAL_RESULTS = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ohr1MulwfyeC",
      "metadata": {
        "id": "ohr1MulwfyeC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Function to see the results\n",
        "def print_summary_table(results):\n",
        "\n",
        "    header = ['Model', 'Dataset', 'Accuracy (%)', 'Total Parameters (M)', 'Train Time (s)', 'Inference Time (ms/img)']\n",
        "    print(f\"{header[0]:<8} | {header[1]:<15} | {header[2]:<15} | {header[3]:<20} | {header[4]:<15} | {header[5]:<25}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for res in results:\n",
        "        print(f\"{res['Model']:<8} | {res['Dataset']:<15} | {res['Accuracy (%)']:<15} | {res['Total Parameters (M)']:<20} | {res['Train Time (s)']:<15} | {res['Inference Time (ms/img)']:<25}\")\n",
        "    print(\"=\"*100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Fim3vbXLKZIU",
      "metadata": {
        "id": "Fim3vbXLKZIU"
      },
      "source": [
        "##### **3.1. CIFAR-10:**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "-cZ0Qeh9nvcW",
      "metadata": {
        "id": "-cZ0Qeh9nvcW"
      },
      "source": [
        "First, we use the Cifar-10 dataset which divide the data into 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "A7khYbZpJ9BJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7khYbZpJ9BJ",
        "outputId": "1364e8a2-affd-464e-a900-82af77428669",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "================ Starting Experiment: CIFAR-10 ================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded CIFAR-10: 50000 training images, 10000 test images.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running ViT on CIFAR-10 ---\n",
            "  Epoch 1/10, Loss: 1.7563\n",
            "  Epoch 2/10, Loss: 1.3345\n",
            "  Epoch 3/10, Loss: 1.2179\n",
            "  Epoch 4/10, Loss: 1.1371\n",
            "  Epoch 5/10, Loss: 1.0780\n",
            "  Epoch 6/10, Loss: 1.0369\n",
            "  Epoch 7/10, Loss: 0.9979\n",
            "  Epoch 8/10, Loss: 0.9674\n",
            "  Epoch 9/10, Loss: 0.9365\n",
            "  Epoch 10/10, Loss: 0.9038\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 3.72 seconds\n",
            "\n",
            "--- Running CNN on CIFAR-10 ---\n",
            "  Epoch 1/10, Loss: 1.3123\n",
            "  Epoch 2/10, Loss: 0.9190\n",
            "  Epoch 3/10, Loss: 0.7618\n",
            "  Epoch 4/10, Loss: 0.6580\n",
            "  Epoch 5/10, Loss: 0.5810\n",
            "  Epoch 6/10, Loss: 0.5276\n",
            "  Epoch 7/10, Loss: 0.4752\n",
            "  Epoch 8/10, Loss: 0.4357\n",
            "  Epoch 9/10, Loss: 0.3950\n",
            "  Epoch 10/10, Loss: 0.3561\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 2.33 seconds\n",
            "Model    | Dataset         | Accuracy (%)    | Total Parameters (M) | Train Time (s)  | Inference Time (ms/img)  \n",
            "----------------------------------------------------------------------------------------------------\n",
            "ViT      | CIFAR-10        | 63.53           | 1.21                 | 333.1           | 0.372                    \n",
            "CNN      | CIFAR-10        | 79.53           | 0.18                 | 191.5           | 0.233                    \n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "DATASET_NAME = 'CIFAR-10'\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# ViT Configuration for CIFAR-10 (32x32 input)\n",
        "IMG_SIZE_C10 = 32\n",
        "PATCH_SIZE = 4 # (32/4)^2 + 1 = 65 tokens\n",
        "\n",
        "print(f\"\\n\\n================ Starting Experiment: {DATASET_NAME} ================\")\n",
        "\n",
        "# 1. Load Data\n",
        "train_loader, test_loader, num_classes = get_cifar_loaders(DATASET_NAME, BATCH_SIZE)\n",
        "\n",
        "# 2. Instantiate Models\n",
        "vit_model = VisionTransformer(\n",
        "    img_size=IMG_SIZE_C10, patch_size=PATCH_SIZE, num_classes=num_classes,\n",
        "    embed_dim=128, depth=6, num_heads=8\n",
        ").to(DEVICE)\n",
        "\n",
        "cnn_model = CustomCNN(num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "# --- ViT Run ---\n",
        "print(f\"\\n--- Running ViT on {DATASET_NAME} ---\")\n",
        "optimizer_vit = optim.Adam(vit_model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_time_vit = train_model(vit_model, train_loader, criterion, optimizer_vit, DEVICE, NUM_EPOCHS)\n",
        "accuracy_vit, inference_time_vit = test_model(vit_model, test_loader, DEVICE,IMG_SIZE_C10)\n",
        "params_vit = count_parameters(vit_model)\n",
        "\n",
        "FINAL_RESULTS.append({\n",
        "    'Model': 'ViT', 'Dataset': DATASET_NAME,\n",
        "    'Accuracy (%)': f\"{accuracy_vit:.2f}\",\n",
        "    'Train Time (s)': f\"{train_time_vit:.1f}\",\n",
        "    'Inference Time (ms/img)': f\"{inference_time_vit:.3f}\",\n",
        "    'Total Parameters (M)': f\"{params_vit/1e6:.2f}\"\n",
        "})\n",
        "\n",
        "# --- CNN Run ---\n",
        "print(f\"\\n--- Running CNN on {DATASET_NAME} ---\")\n",
        "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
        "train_time_cnn = train_model(cnn_model, train_loader, criterion, optimizer_cnn, DEVICE, NUM_EPOCHS)\n",
        "accuracy_cnn, inference_time_cnn = test_model(cnn_model, test_loader, DEVICE, IMG_SIZE_C10)\n",
        "params_cnn = count_parameters(cnn_model)\n",
        "\n",
        "FINAL_RESULTS.append({\n",
        "    'Model': 'CNN', 'Dataset': DATASET_NAME,\n",
        "    'Accuracy (%)': f\"{accuracy_cnn:.2f}\",\n",
        "    'Train Time (s)': f\"{train_time_cnn:.1f}\",\n",
        "    'Inference Time (ms/img)': f\"{inference_time_cnn:.3f}\",\n",
        "    'Total Parameters (M)': f\"{params_cnn/1e6:.2f}\"\n",
        "})\n",
        "\n",
        "print_summary_table(FINAL_RESULTS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "OLXgi-2noJRm",
      "metadata": {
        "id": "OLXgi-2noJRm"
      },
      "source": [
        "We can observe clearly that for the Cifar-10 dataset, the CNN is better than ViT in both accuracy, training time and inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "KVIgUp0_rDc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVIgUp0_rDc9",
        "outputId": "b5488926-eb0c-4bea-ecd8-edc517ffc9a2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VisionTransformer(\n",
            "  (patch_embed): PatchEmbedding(\n",
            "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
            "  (blocks): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
            "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(vit_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "K8biGtLDrGNc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8biGtLDrGNc",
        "outputId": "3ef6a91d-7cc2-473b-8673-4a921feba34c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CustomCNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "  )\n",
            "  (layer1): Sequential(\n",
            "    (0): CNNBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): CNNBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): CNNBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): CNNBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): CNNBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): CNNBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(cnn_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "LtyTgOkFKmWx",
      "metadata": {
        "id": "LtyTgOkFKmWx"
      },
      "source": [
        "##### **3.2. CIFAR-100**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Sj3yxmyeoZkn",
      "metadata": {
        "id": "Sj3yxmyeoZkn"
      },
      "source": [
        "Next, we will test our architectures on the Cifar-100 datasets, which includes labelled data of 100 different classes. This dataset have much more classes than the Cifar-10 but it has the same number of training images (50000) and test images (10000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "xaouPrRDJ_UY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaouPrRDJ_UY",
        "outputId": "4636b234-237d-469e-bda0-92fdb1b9eb78",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "================ Starting Experiment: CIFAR-100 ================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:04<00:00, 39.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded CIFAR-100: 50000 training images, 10000 test images.\n",
            "\n",
            "--- Running ViT on CIFAR-100 ---\n",
            "  Epoch 1/10, Loss: 3.8276\n",
            "  Epoch 2/10, Loss: 3.1781\n",
            "  Epoch 3/10, Loss: 2.8948\n",
            "  Epoch 4/10, Loss: 2.7227\n",
            "  Epoch 5/10, Loss: 2.5768\n",
            "  Epoch 6/10, Loss: 2.4473\n",
            "  Epoch 7/10, Loss: 2.3319\n",
            "  Epoch 8/10, Loss: 2.2427\n",
            "  Epoch 9/10, Loss: 2.1432\n",
            "  Epoch 10/10, Loss: 2.0505\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 3.06 seconds\n",
            "\n",
            "--- Running CNN on CIFAR-100 ---\n",
            "  Epoch 1/10, Loss: 3.8297\n",
            "  Epoch 2/10, Loss: 3.1520\n",
            "  Epoch 3/10, Loss: 2.7438\n",
            "  Epoch 4/10, Loss: 2.4638\n",
            "  Epoch 5/10, Loss: 2.2652\n",
            "  Epoch 6/10, Loss: 2.1058\n",
            "  Epoch 7/10, Loss: 1.9783\n",
            "  Epoch 8/10, Loss: 1.8711\n",
            "  Epoch 9/10, Loss: 1.7821\n",
            "  Epoch 10/10, Loss: 1.6947\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 2.35 seconds\n",
            "Model    | Dataset         | Accuracy (%)    | Total Parameters (M) | Train Time (s)  | Inference Time (ms/img)  \n",
            "----------------------------------------------------------------------------------------------------\n",
            "ViT      | CIFAR-10        | 63.53           | 1.21                 | 333.1           | 0.372                    \n",
            "CNN      | CIFAR-10        | 79.53           | 0.18                 | 191.5           | 0.233                    \n",
            "ViT      | CIFAR-100       | 39.32           | 1.22                 | 326.0           | 0.306                    \n",
            "CNN      | CIFAR-100       | 43.64           | 0.18                 | 191.4           | 0.235                    \n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "DATASET_NAME = 'CIFAR-100'\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# ViT Configuration for CIFAR-10 (32x32 input)\n",
        "IMG_SIZE_C100 = 32\n",
        "PATCH_SIZE = 4 # (32/4)^2 + 1 = 65 tokens\n",
        "\n",
        "print(f\"\\n\\n================ Starting Experiment: {DATASET_NAME} ================\")\n",
        "\n",
        "# 1. Load Data\n",
        "train_loader, test_loader, num_classes = get_cifar_loaders(DATASET_NAME, BATCH_SIZE)\n",
        "\n",
        "# 2. Instantiate Models\n",
        "vit_model = VisionTransformer(\n",
        "    img_size=IMG_SIZE_C100, patch_size=PATCH_SIZE, num_classes=num_classes,\n",
        "    embed_dim=128, depth=6, num_heads=8\n",
        ").to(DEVICE)\n",
        "\n",
        "cnn_model = CustomCNN(num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "# --- ViT Run ---\n",
        "print(f\"\\n--- Running ViT on {DATASET_NAME} ---\")\n",
        "optimizer_vit = optim.Adam(vit_model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_time_vit = train_model(vit_model, train_loader, criterion, optimizer_vit, DEVICE, NUM_EPOCHS)\n",
        "accuracy_vit, inference_time_vit = test_model(vit_model, test_loader, DEVICE, IMG_SIZE_C100)\n",
        "params_vit = count_parameters(vit_model)\n",
        "\n",
        "FINAL_RESULTS.append({\n",
        "    'Model': 'ViT', 'Dataset': DATASET_NAME,\n",
        "    'Accuracy (%)': f\"{accuracy_vit:.2f}\",\n",
        "    'Train Time (s)': f\"{train_time_vit:.1f}\",\n",
        "    'Inference Time (ms/img)': f\"{inference_time_vit:.3f}\",\n",
        "    'Total Parameters (M)': f\"{params_vit/1e6:.2f}\"\n",
        "})\n",
        "\n",
        "# --- CNN Run ---\n",
        "print(f\"\\n--- Running CNN on {DATASET_NAME} ---\")\n",
        "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
        "train_time_cnn = train_model(cnn_model, train_loader, criterion, optimizer_cnn, DEVICE, NUM_EPOCHS)\n",
        "accuracy_cnn, inference_time_cnn = test_model(cnn_model, test_loader, DEVICE, IMG_SIZE_C100)\n",
        "params_cnn = count_parameters(cnn_model)\n",
        "\n",
        "FINAL_RESULTS.append({\n",
        "    'Model': 'CNN', 'Dataset': DATASET_NAME,\n",
        "    'Accuracy (%)': f\"{accuracy_cnn:.2f}\",\n",
        "    'Train Time (s)': f\"{train_time_cnn:.1f}\",\n",
        "    'Inference Time (ms/img)': f\"{inference_time_cnn:.3f}\",\n",
        "    'Total Parameters (M)': f\"{params_cnn/1e6:.2f}\"\n",
        "})\n",
        "print_summary_table(FINAL_RESULTS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "L7mw_rL_ovPP",
      "metadata": {
        "id": "L7mw_rL_ovPP"
      },
      "source": [
        "For this dataset, the result is still the same with the Cifar-10. Although the accuracy is lower than the Cifar-10 dataset (because we have more classes and we just train on 10 epoches and the architecture are not complex enough), the CNN is still better in accuracy, training time and inference time."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2lHyveKyKvFp",
      "metadata": {
        "id": "2lHyveKyKvFp"
      },
      "source": [
        "##### **3.3. Image-net 200:**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ypW4pgiB6yeh",
      "metadata": {
        "id": "ypW4pgiB6yeh"
      },
      "source": [
        "Lastly, we use the Image-net 200 dataset. This is a small version of Image-net dataset, which has 200 classes. This dataset have 42905 training images and 10000 test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "93f1bdb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93f1bdb6",
        "outputId": "7e9aadb5-faca-4a90-e720-d420eadaaf1f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "================ Starting Experiment: ImageNet-200 ================\n",
            "Using Colab cache for faster access to the 'tiny-imagenet' dataset.\n",
            "Writable directory already exists. Skipping copy.\n",
            "Sorting Tiny ImageNet validation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing validation images: 100%|██████████| 10000/10000 [00:00<00:00, 81404.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set sorting complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ImageNet-200: 42905 training images, 10000 test images.\n",
            "\n",
            "--- Running ViT on ImageNet-200 ---\n",
            "  Epoch 1/10, Loss: 3.9440\n",
            "  Epoch 2/10, Loss: 3.4167\n",
            "  Epoch 3/10, Loss: 3.1500\n",
            "  Epoch 4/10, Loss: 2.9503\n",
            "  Epoch 5/10, Loss: 2.7890\n",
            "  Epoch 6/10, Loss: 2.6675\n",
            "  Epoch 7/10, Loss: 2.5469\n",
            "  Epoch 8/10, Loss: 2.4371\n",
            "  Epoch 9/10, Loss: 2.3363\n",
            "  Epoch 10/10, Loss: 2.2429\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 4.51 seconds\n",
            "\n",
            "--- Running CNN on ImageNet-200 ---\n",
            "  Epoch 1/10, Loss: 3.7667\n",
            "  Epoch 2/10, Loss: 3.2050\n",
            "  Epoch 3/10, Loss: 2.9647\n",
            "  Epoch 4/10, Loss: 2.7883\n",
            "  Epoch 5/10, Loss: 2.6382\n",
            "  Epoch 6/10, Loss: 2.5159\n",
            "  Epoch 7/10, Loss: 2.4068\n",
            "  Epoch 8/10, Loss: 2.3192\n",
            "  Epoch 9/10, Loss: 2.2412\n",
            "  Epoch 10/10, Loss: 2.1686\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 4.74 seconds\n"
          ]
        }
      ],
      "source": [
        "DATASET_NAME = 'ImageNet-200'\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# ViT Configuration for ImageNet-200 (64x64 input)\n",
        "IMG_SIZE_200 = 64\n",
        "PATCH_SIZE = 8 # (64/8)^2 + 1 = 65 tokens\n",
        "\n",
        "print(f\"\\n\\n================ Starting Experiment: {DATASET_NAME} ================\")\n",
        "\n",
        "# 1. Download and Prepare Data\n",
        "TINY_IMAGENET_ROOT = None\n",
        "try:\n",
        "    TINY_IMAGENET_PATH = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n",
        "    TINY_IMAGENET_ROOT = os.path.join(TINY_IMAGENET_PATH, 'tiny-imagenet-200')\n",
        "    if not os.path.isdir(TINY_IMAGENET_ROOT):\n",
        "        TINY_IMAGENET_ROOT = TINY_IMAGENET_PATH\n",
        "except Exception as e:\n",
        "    print(f\"KaggleHub download failed. Error: {e}\")\n",
        "\n",
        "WRITABLE_DATA_ROOT = None\n",
        "if TINY_IMAGENET_ROOT:\n",
        "    WRITABLE_DATA_ROOT = prepare_writable_data(TINY_IMAGENET_ROOT)\n",
        "\n",
        "if WRITABLE_DATA_ROOT is None:\n",
        "    print(\"Skipping ImageNet-200 experiment due to data preparation failure.\")\n",
        "else:\n",
        "    # 2. Load Data (Sorting happens inside this call)\n",
        "    train_loader, test_loader, num_classes = get_imagenet_200_loaders(WRITABLE_DATA_ROOT, BATCH_SIZE)\n",
        "\n",
        "    if train_loader is None:\n",
        "\n",
        "        print(\"Data loading failed.\")\n",
        "    else:\n",
        "        # 3. Instantiate Models\n",
        "        vit_model = VisionTransformer(\n",
        "            img_size=IMG_SIZE_200, patch_size=PATCH_SIZE, num_classes=num_classes,\n",
        "            embed_dim=128, depth=6, num_heads=8\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        cnn_model = CustomCNN(num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "        # --- ViT Run ---\n",
        "        print(f\"\\n--- Running ViT on {DATASET_NAME} ---\")\n",
        "        optimizer_vit = optim.Adam(vit_model.parameters(), lr=LEARNING_RATE)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        train_time_vit = train_model(vit_model, train_loader, criterion, optimizer_vit, DEVICE, NUM_EPOCHS)\n",
        "        accuracy_vit, inference_time_vit = test_model(vit_model, test_loader, DEVICE, IMG_SIZE_200)\n",
        "        params_vit = count_parameters(vit_model)\n",
        "\n",
        "        FINAL_RESULTS.append({\n",
        "            'Model': 'ViT', 'Dataset': DATASET_NAME,\n",
        "            'Accuracy (%)': f\"{accuracy_vit:.2f}\",\n",
        "            'Train Time (s)': f\"{train_time_vit:.1f}\",\n",
        "            'Inference Time (ms/img)': f\"{inference_time_vit:.3f}\",\n",
        "            'Total Parameters (M)': f\"{params_vit/1e6:.2f}\"\n",
        "        })\n",
        "\n",
        "        # --- CNN Run ---\n",
        "        print(f\"\\n--- Running CNN on {DATASET_NAME} ---\")\n",
        "        optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
        "        train_time_cnn = train_model(cnn_model, train_loader, criterion, optimizer_cnn, DEVICE, NUM_EPOCHS)\n",
        "        accuracy_cnn, inference_time_cnn = test_model(cnn_model, test_loader, DEVICE, IMG_SIZE_200)\n",
        "        params_cnn = count_parameters(cnn_model)\n",
        "\n",
        "        FINAL_RESULTS.append({\n",
        "            'Model': 'CNN', 'Dataset': DATASET_NAME,\n",
        "            'Accuracy (%)': f\"{accuracy_cnn:.2f}\",\n",
        "            'Train Time (s)': f\"{train_time_cnn:.1f}\",\n",
        "            'Inference Time (ms/img)': f\"{inference_time_cnn:.3f}\",\n",
        "            'Total Parameters (M)': f\"{params_cnn/1e6:.2f}\"\n",
        "        })\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "DMx9CsGBMWEp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMx9CsGBMWEp",
        "outputId": "543972dc-36d7-4696-e0b5-545f93d6d98c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model    | Dataset         | Accuracy (%)    | Total Parameters (M) | Train Time (s)  | Inference Time (ms/img)  \n",
            "----------------------------------------------------------------------------------------------------\n",
            "ViT      | CIFAR-10        | 63.53           | 1.21                 | 333.1           | 0.372                    \n",
            "CNN      | CIFAR-10        | 79.53           | 0.18                 | 191.5           | 0.233                    \n",
            "ViT      | CIFAR-100       | 39.32           | 1.22                 | 326.0           | 0.306                    \n",
            "CNN      | CIFAR-100       | 43.64           | 0.18                 | 191.4           | 0.235                    \n",
            "ViT      | ImageNet-200    | 0.55            | 1.25                 | 297.2           | 0.451                    \n",
            "CNN      | ImageNet-200    | 0.41            | 0.19                 | 297.9           | 0.474                    \n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print_summary_table(FINAL_RESULTS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "CSZ7rJX__y7q",
      "metadata": {
        "id": "CSZ7rJX__y7q"
      },
      "source": [
        "For the Imgaenet-200, we can observe clearly that the ViT is better than the CNN in the accuracy, training time and the inference time. This result is coherent with the theory that for a dataset with a huge number of classes, the ViT outperforms the CNN."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b78850bf",
      "metadata": {
        "id": "b78850bf"
      },
      "source": [
        "### **Summary of CNNs and ViTs**\n",
        "\n",
        "| Features | CNNs | ViTs |\n",
        "| :--- | :--- | :--- |\n",
        "| **Attention Scope** | Capture local features via convolutions | Capture global relationships via self-attention |\n",
        "| **Inductive Bias** | Strong biases (locality, translation invariance) | Minimal biases, more flexible but data-hungry |\n",
        "| **Data Requirement** | Work well with small datasets | Need large datasets for best performance |\n",
        "| **Feature Learning** | Learn hierarchical features | Learn context-rich, long-range features |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "56c69f27",
      "metadata": {
        "id": "56c69f27"
      },
      "source": [
        "### **Comparative Analysis and Conclusion**\n",
        "\n",
        "#### **Performance on Small Datasets (e.g., CIFAR-10, CIFAR-100)**\n",
        "\n",
        "For datasets characterized by limited data volume and a small number of classes, the Convolutional Neural Network (CNN) architecture demonstrates superior performance across multiple metrics compared to the Vision Transformer (ViT).\n",
        "\n",
        "*   **Performance:** CNNs consistently achieve higher classification accuracy.\n",
        "*   **Efficiency:** CNNs exhibit significantly shorter training times and lower inference latency.\n",
        "*   **Cost:** The CNN architecture requires a substantially smaller number of trainable parameters, resulting in a lower memory footprint and computational cost.\n",
        "\n",
        "This outcome is attributed to the strong **inductive biases** (locality and translation invariance) inherent in CNNs, which enable effective feature learning from limited data.\n",
        "\n",
        "#### **Performance on Large-Scale Datasets (e.g., ImageNet-200)**\n",
        "\n",
        "When transitioning to large datasets featuring high data volume and a large number of classes, the comparative advantage shifts:\n",
        "\n",
        "*   **Accuracy:** ViT architectures generally achieve superior classification accuracy, demonstrating their ability to leverage extensive data to learn complex, global feature representations without relying on local biases.\n",
        "*   **Efficiency:** While ViTs are often computationally slower than optimized CNNs, the difference in training and inference time becomes less pronounced or even slightly favors the ViT in certain highly optimized implementations, particularly when considering the superior accuracy achieved.\n",
        "*   **Cost and Resource Allocation:** A major drawback of the ViT is its **high parameter count**. ViTs typically require a significantly larger number of parameters than comparable CNNs (e.g., ResNet variants) to achieve peak performance. This translates directly to higher memory requirements and greater computational expense during both training and deployment.\n",
        "\n",
        "#### **Conclusion on Architectural Choice**\n",
        "\n",
        "The choice between a CNN and a ViT should be dictated by the available resources and the project's primary objective:\n",
        "\n",
        "1.  **Resource-Constrained Environments:** If computational budget, memory constraints, or training time are critical factors, the **CNN architecture (e.g., ResNet)** remains the preferred and most resource-efficient choice, often providing acceptable accuracy with minimal cost.\n",
        "2.  **Accuracy-Driven Objectives:** If the primary goal is maximizing classification accuracy and resources are abundant, the **Vision Transformer** is the superior choice, as its global attention mechanism allows it to achieve state-of-the-art results by effectively modeling long-range dependencies in large datasets. The higher computational cost is accepted as a necessary trade-off for improved performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZWDtsb1FCf0K",
      "metadata": {
        "id": "ZWDtsb1FCf0K",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
