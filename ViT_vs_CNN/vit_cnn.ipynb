{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0b8a944a",
      "metadata": {
        "id": "0b8a944a"
      },
      "source": [
        "## Project: ViT vs CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JR6I7my9CF9H",
      "metadata": {
        "id": "JR6I7my9CF9H"
      },
      "source": [
        "The objective of this project is to compare the performances of two different model in the task of image classification: Vision Transformers (ViT) an Convolutional Neural Network (CNN). In this project, we will use three different labelled datasets for comparision: CIFAR-10 (10 classes), CIFAR-100 (100 classes) and Imagenet-200 (200 classes)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99bb45f7",
      "metadata": {
        "id": "99bb45f7"
      },
      "source": [
        "This project use information from these sources:\n",
        "\n",
        "_ An Image is Worth 16x16 Words Transformers for Image Recognition at Scale, Vision Transformer, ViT, by Google Research, Brain Team 2021 ICLR. https://arxiv.org/abs/2010.11929\n",
        "\n",
        "_ https://www.geeksforgeeks.org/deep-learning/vision-transformer-vit-architecture/\n",
        "\n",
        "_ https://sh-tsang.medium.com/review-vision-transformer-vit-406568603de0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "laowmO1QMma5",
      "metadata": {
        "id": "laowmO1QMma5"
      },
      "source": [
        "##### **1. Prepare the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1fb02427",
      "metadata": {
        "id": "1fb02427"
      },
      "outputs": [],
      "source": [
        "# Import necessary library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import kagglehub\n",
        "import shutil\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "uRZgI6cUZXUi",
      "metadata": {
        "id": "uRZgI6cUZXUi"
      },
      "outputs": [],
      "source": [
        "def prepare_writable_data(read_only_dir):\n",
        "    \"\"\"Copies the dataset from the read-only input path to the writable working path.\"\"\"\n",
        "\n",
        "    if read_only_dir is None or not os.path.exists(read_only_dir):\n",
        "        return None\n",
        "\n",
        "    # Define the destination path in the writable working directory\n",
        "    writable_dir = os.path.join('/kaggle/working/', 'tiny-imagenet-writable')\n",
        "\n",
        "    if os.path.exists(writable_dir):\n",
        "        print(\"Writable directory already exists. Skipping copy.\")\n",
        "        return writable_dir\n",
        "\n",
        "    print(f\"Copying data from {read_only_dir} to {writable_dir}...\")\n",
        "\n",
        "    try:\n",
        "        # Use copytree to copy the entire directory structure\n",
        "        shutil.copytree(read_only_dir, writable_dir)\n",
        "        print(\"Data copied successfully.\")\n",
        "        return writable_dir\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data copying: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8f1072b6",
      "metadata": {
        "id": "8f1072b6"
      },
      "outputs": [],
      "source": [
        "def sort_tiny_imagenet_validation(data_dir):\n",
        "    \"\"\"\n",
        "    Sorts the validation images of Tiny ImageNet into class-specific folders.\n",
        "    This is necessary because the raw dataset dumps all validation images\n",
        "    into a single folder, while ImageFolder expects class subdirectories.\n",
        "    \"\"\"\n",
        "\n",
        "    val_dir = os.path.join(data_dir, 'val')\n",
        "    val_images_dir = os.path.join(val_dir, 'images')\n",
        "    annotations_file = os.path.join(val_dir, 'val_annotations.txt')\n",
        "\n",
        "    print(\"Sorting Tiny ImageNet validation set...\")\n",
        "\n",
        "    # 1. Read annotations and create class directories\n",
        "    with open(annotations_file, 'r') as f:\n",
        "        annotations = f.readlines()\n",
        "\n",
        "    # 2. Process each annotation line\n",
        "    for line in tqdm(annotations, desc=\"Processing validation images\"):\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        filename = parts[0]  # e.g., 'val_0.JPEG'\n",
        "        synset_id = parts[1] # e.g., 'n01440764'\n",
        "\n",
        "        # Define source and destination paths\n",
        "        src_path = os.path.join(val_images_dir, filename)\n",
        "        dst_dir = os.path.join(val_dir, synset_id)\n",
        "        dst_path = os.path.join(dst_dir, filename)\n",
        "\n",
        "        # Create the destination class directory if it doesn't exist\n",
        "        os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "        # Move the image\n",
        "        if os.path.exists(src_path):\n",
        "            shutil.move(src_path, dst_path)\n",
        "\n",
        "    # 3. Clean up the original 'images' folder and annotations file\n",
        "    if os.path.exists(val_images_dir):\n",
        "        try:\n",
        "            os.rmdir(val_images_dir)\n",
        "        except OSError:\n",
        "            # Directory might not be empty if some files failed to move\n",
        "            print(\"Warning: Could not remove original 'val/images' directory.\")\n",
        "\n",
        "    print(\"Validation set sorting complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "853e1ea3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "853e1ea3",
        "outputId": "5fba7f1f-e301-4685-993a-bd7a4204b3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download Tiny ImageNet via KaggleHub...\n",
            "Using Colab cache for faster access to the 'tinyimagenet200' dataset.\n",
            "Tiny ImageNet downloaded successfully to: /kaggle/input/tinyimagenet200\n"
          ]
        }
      ],
      "source": [
        "print(\"Attempting to download Tiny ImageNet via KaggleHub...\")\n",
        "\n",
        "# This returns the local path where the dataset files are stored\n",
        "TINY_IMAGENET_PATH = kagglehub.dataset_download(\"nikhilshingadiya/tinyimagenet200\")\n",
        "TINY_IMAGENET_ROOT = None\n",
        "WRITABLE_DATA_ROOT = None\n",
        "print(f\"Tiny ImageNet downloaded successfully to: {TINY_IMAGENET_PATH}\")\n",
        "\n",
        "TINY_IMAGENET_ROOT = os.path.join(TINY_IMAGENET_PATH, 'tiny-imagenet-200')\n",
        "if not os.path.isdir(TINY_IMAGENET_ROOT):\n",
        "    # If the structure is flat, the path itself might be the root\n",
        "    TINY_IMAGENET_ROOT = TINY_IMAGENET_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa9b13ef",
      "metadata": {
        "id": "fa9b13ef"
      },
      "source": [
        "#### **2. Model architecture:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebcd27c0",
      "metadata": {
        "id": "ebcd27c0"
      },
      "source": [
        "##### **2.1 Vision Transformer (ViT) Architecture Overview**\n",
        "\n",
        "![Vision Transformer Architecture](ViTarchi.png)\n",
        "\n",
        "##### **2.1 Vision Transformer (ViT) Architecture Overview**\n",
        "\n",
        "![Vision Transformer Architecture](ViTarchi.png)\n",
        "\n",
        "The Vision Transformer (ViT) adapts the Transformer architecture from natural language processing to computer vision by representing images as sequences of visual tokens, similar to words in a sentence. Its design consists of several key components, each contributing to effective image representation and classification:\n",
        "\n",
        "\n",
        "**1. Image Patching and embedding:**\n",
        "\n",
        "In this stage, the ViT will convert a 2D image into a sequence of fixed-size, non-overlapping patches, with the same idea of the tokens in NLP (Natural language processing). Each patch of size $P \\times P \\times C$   is flattened into a one-dimensional vector of length\n",
        "$P^2 \\times C$ . These vectors are then projected into a shared D-dimensional embedding space using a learnable linear transformation, enabling the model to extract high-level visual features.\n",
        "\n",
        "**2. Positional Encoding**\n",
        "\n",
        "Since Transformers are inherently permutation invariant, positional information must be explicitly provided. ViT adds learnable positional embeddings to the patch embeddings to encode the spatial arrangement of patches within the image. These embeddings allow the model to understand relative and absolute patch positions and adapt more flexibly to different image resolutions compared to fixed positional encodings.\n",
        "\n",
        "**3. Adding the Classification Token (CLS Token)**\n",
        "\n",
        "A learnable classification token (CLS) is prepended to the sequence of patch embeddings. This token aggregates information from all patches through self-attention and serves as a global image representation. Unlike CNNs, which rely on pooling operations, ViT uses the final representation of the CLS token directly for classification.\n",
        "\n",
        "**4. Transformer Encoder (Pre-LayerNorm Architecture)**\n",
        "\n",
        "![Pre-LayerNorm](prelayer_norm.png)\n",
        "\n",
        "The core of ViT consists of stacked Transformer encoder blocks following a pre-layer normalization (Pre-LN) design. In this setup, LayerNorm is applied before both the multi-head self-attention and feed-forward sublayers, improving gradient stability and enabling deeper architectures.\n",
        "\n",
        "$\\mathrm{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma} \\odot \\gamma + \\beta$\n",
        "where\n",
        "*  $\\mu, \\sigma $ are mean and std across features\n",
        "\n",
        "* $\\gamma, \\beta $ are learnable\n",
        "\n",
        "\n",
        "Each encoder block includes multi-head self-attention, a feed-forward network, residual connections, and layer normalization.\n",
        "\n",
        "\n",
        "**5. Multi-Head Self -Attention (MSA)**\n",
        "\n",
        "Self-attention allows each patch to interact with every other patch, capturing long-range dependencies across the image. Queries, keys, and values are computed through linear projections of the input embeddings, and attention scores are obtained using scaled dot-product attention.\n",
        "\n",
        "By using multiple attention heads, the model can focus on different aspects of the image simultaneously—such as textures, edges, colors, or global structure—leading to richer feature representations. The outputs of all heads are concatenated and linearly projected to form the final attention output.\n",
        "\n",
        "**6. Feed-Forward Network (FFN)**\n",
        "\n",
        "Following the attention block, each token is processed independently by a feed-forward network consisting of two fully connected layers with a GELU activation in between. This network expands the embedding dimension and then projects it back, enabling non-linear transformations that enhance representational capacity while sharing weights across tokens.\n",
        "\n",
        "$\\mathrm{FFN}(x) = W_2\\, \\mathrm{GELU}(W_1 x + b_1) + b_2$\n",
        "\n",
        "**7. Residual Conections and Layer Normalization**\n",
        "\n",
        "Residual (skip) connections are used throughout the encoder to preserve information from earlier layers and prevent performance degradation in deep networks. Layer normalization stabilizes training by normalizing feature distributions, while the pre-LN design ensures well-conditioned gradients and consistent scaling across layers.\n",
        "\n",
        "**8. Classification Head (MLP Head)**\n",
        "\n",
        "The final representation of the CLS token is passed through a small multi-layer perceptron (MLP) head to produce class logits. A softmax function converts these logits into class probabilities, enabling multi-class classification and training with cross-entropy loss.\n",
        "\n",
        "**9. Training Vision Transformers**\n",
        "\n",
        "Unlike CNNs, ViTs have minimal inductive bias, as they do not inherently encode locality or translation invariance. As a result, they typically require large-scale datasets and strong data augmentation to generalize well. To address this, ViTs are often pretrained on large datasets—using supervised or self-supervised methods—before being fine-tuned on downstream tasks with fewer labeled samples. Fine-tuning commonly employs techniques such as layer-wise learning rate decay to improve performance and stability.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "420f1402",
      "metadata": {
        "id": "420f1402"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Patch Embedding (Tokenization):\n",
        "# This modules is to convert the original 2D images into\n",
        "# 1D sequence of embeded vectors.\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        # Conv2d implements the non-overlapping patch embedding/linear projection\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W) -> (B, D, H', W')\n",
        "        x = self.proj(x)\n",
        "        # Flatten H' x W' into sequence length N, transpose to (B, N, D)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "158f5430",
      "metadata": {
        "id": "158f5430"
      },
      "outputs": [],
      "source": [
        "# --- Vision Transformer (ViT) Model ---\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "                 embed_dim=128, depth=6, num_heads=8, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=0.1)\n",
        "\n",
        "        # Define a single standard Transformer Encoder Layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "            dropout=0.1,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        # Stack the layers using nn.TransformerEncoder\n",
        "        self.blocks = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = self.pos_drop(x + self.pos_embed)\n",
        "\n",
        "        # The standard PyTorch TransformerEncoder handles the sequence of blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Classification uses the CLS token output\n",
        "        cls_output = x[:, 0]\n",
        "        x = self.head(cls_output)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TmZB5L8lm9eb",
      "metadata": {
        "id": "TmZB5L8lm9eb"
      },
      "source": [
        "##### **2.2 CNN architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Az15m3jnnCgp",
      "metadata": {
        "id": "Az15m3jnnCgp"
      },
      "source": [
        "In this project, we will build a CNN architecture inspired by ResNet, which is highly effective for image classification.\n",
        "\n",
        "This model uses $3 \\times 3$ convolutions and gradually increases the channel depth while reducing the spatial resolution via stride-2 convolutions in the main blocks.\n",
        "\n",
        "###### **2.2.1 Structure of the Core Building Block (`CNNBlock`)**\n",
        "\n",
        "The `CNNBlock` implements the Basic Block structure from ResNet, ensuring efficient training even with limited depth.\n",
        "\n",
        "$$\n",
        "\\text{Output} = \\text{ReLU}(\\text{BN}(\\text{Conv}_2(\\text{ReLU}(\\text{BN}(\\text{Conv}_1(\\text{Input})))) + \\text{Shortcut}(\\text{Input}))\n",
        "$$\n",
        "\n",
        "| Step | Operation | Output Size |\n",
        "| :--- | :--- | :--- |\n",
        "| **Input** | Feature Map | $H \\times W \\times C_{in}$ |\n",
        "| **Conv 1** | Conv $3 \\times 3$, Stride $S$, Padding 1 | $H/S \\times W/S \\times C_{out}$ |\n",
        "| **Conv 2** | Conv $3 \\times 3$, Stride 1, Padding 1 | $H/S \\times W/S \\times C_{out}$ |\n",
        "| **Shortcut** | Conv $1 \\times 1$ (if stride $\\neq 1$ or $C_{in} \\neq C_{out}$) | $H/S \\times W/S \\times C_{out}$ |\n",
        "| **Output** | Output Feature Map (after addition and ReLU) | $H/S \\times W/S \\times C_{out}$ |\n",
        "\n",
        "---\n",
        "\n",
        "###### **2.2.2 Full Model Architecture (`CustomCNN`)**\n",
        "\n",
        "Assuming an input image size of **$32 \\times 32$** (for CIFAR), here is the layer breakdown:\n",
        "\n",
        "| Layer Name | Module/Operation | Stride | Output Channels (Depth) | Output Size (H x W) | Notes |\n",
        "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
        "| **Input** | Image | - | 3 | $32 \\times 32$ | |\n",
        "| **Conv 1** | Conv $3 \\times 3$ + BN + ReLU | 1 | 16 | $32 \\times 32$ | Initial Feature Map |\n",
        "| **Layer 1** | **2x** `CNNBlock` | 1 | 16 | $32 \\times 32$ | No spatial reduction |\n",
        "| **Layer 2** | **2x** `CNNBlock` | 2 (in first block) | 32 | $16 \\times 16$ | Spatial Downsampling (32 $\\to$ 16) |\n",
        "| **Layer 3** | **2x** `CNNBlock` | 2 (in first block) | 64 | $8 \\times 8$ | Spatial Downsampling (16 $\\to$ 8) |\n",
        "| **Avg Pool** | AdaptiveAvgPool2d | - | 64 | $1 \\times 1$ | Global Pooling |\n",
        "| **Linear** | Fully Connected | - | Num Classes | 1 | Final Classification |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "501de026",
      "metadata": {
        "id": "501de026"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        # 3x3 Conv, BN, ReLU\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        # 3x3 Conv, BN (Residual part)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Shortcut connection for ResNet-like structure\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        result = self.bn2(self.conv2(result))\n",
        "        result += self.shortcut(x)\n",
        "        result = nn.ReLU()(result)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a4c732ae",
      "metadata": {
        "id": "a4c732ae"
      },
      "outputs": [],
      "source": [
        "# --- Custom CNN Model ---\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_channels = 16\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # ResNet-like blocks (16 -> 32 -> 64 channels)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(CNNBlock(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a133fe25",
      "metadata": {
        "id": "a133fe25"
      },
      "source": [
        "#### 2 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the accuracy and effectiveness of the ViT architecture, we will use some data augmentation (translation, rotation, flip) to load the training data."
      ],
      "metadata": {
        "id": "O_9cjSBaD-3i"
      },
      "id": "O_9cjSBaD-3i"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "97fa7518",
      "metadata": {
        "id": "97fa7518"
      },
      "outputs": [],
      "source": [
        "def get_cifar_loaders(dataset_name, batch_size=64):\n",
        "    \"\"\"Downloads and prepares data loaders for CIFAR-10 or CIFAR-100 with augmentation.\"\"\"\n",
        "    if dataset_name == 'CIFAR-10':\n",
        "        dataset_class = datasets.CIFAR10\n",
        "        num_classes = 10\n",
        "    elif dataset_name == 'CIFAR-100':\n",
        "        dataset_class = datasets.CIFAR100\n",
        "        num_classes = 100\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
        "\n",
        "    #Augmentation for Training Data (32x32)\n",
        "    train_transform = transforms.Compose([\n",
        "        # Random Augmentations\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10), # Rotate up to 10 degrees\n",
        "        transforms.RandomAffine(\n",
        "            degrees=0,\n",
        "            translate=(0.1, 0.1) # Translate up to 10% horizontally/vertically\n",
        "        ),\n",
        "        # Final steps\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Standard Transform for Test Data (No augmentation)\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    train_data = dataset_class(root='./data', train=True, download=True, transform=train_transform)\n",
        "    test_data = dataset_class(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(f\"Loaded {dataset_name}: {len(train_data)} training images, {len(test_data)} test images (with augmentation).\")\n",
        "\n",
        "    return train_loader, test_loader, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a2Zdgr8O74YL",
      "metadata": {
        "id": "a2Zdgr8O74YL"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFile\n",
        "\n",
        "# Crucial setting for handling truncated images (common in ImageNet)\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def is_valid_image_file(path):\n",
        "    \"\"\"Checks if a file is a valid, non-corrupted image.\"\"\"\n",
        "    try:\n",
        "        # 1. Check if file is empty\n",
        "        if os.path.getsize(path) == 0:\n",
        "            return False\n",
        "\n",
        "        # 2. Attempt to open and verify the image header\n",
        "        img = Image.open(path)\n",
        "        img.verify() # Verify the file integrity\n",
        "        return True\n",
        "    except Exception:\n",
        "        # If PIL throws any error (UnidentifiedImageError, IOError, etc.), it's invalid\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e09fa825",
      "metadata": {
        "id": "e09fa825"
      },
      "outputs": [],
      "source": [
        "def get_imagenet_200_loaders(data_dir, batch_size=64):\n",
        "    \"\"\"\n",
        "    Loads ImageNet-200 (Tiny ImageNet) data with augmentation.\n",
        "    \"\"\"\n",
        "    if data_dir is None:\n",
        "        print(\"Data directory is invalid. Cannot load data.\")\n",
        "        return None, None, 200, None\n",
        "\n",
        "    # Ensure validation data is structured correctly\n",
        "    sort_tiny_imagenet_validation(data_dir)\n",
        "\n",
        "    #Augmentation for Training Data (64x64)\n",
        "    train_transform = transforms.Compose([\n",
        "        # Initial resizing/cropping\n",
        "        transforms.Resize(64),\n",
        "        transforms.CenterCrop(64),\n",
        "        # Random Augmentations\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15), # Slightly more rotation\n",
        "        transforms.RandomAffine(\n",
        "            degrees=0,\n",
        "            translate=(0.15, 0.15) # Slightly more translation\n",
        "        ),\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    #Standard Transform for Test Data\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize(64),\n",
        "        transforms.CenterCrop(64),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        train_data = datasets.ImageFolder(\n",
        "            root=os.path.join(data_dir, 'train'),\n",
        "            transform=train_transform,\n",
        "            is_valid_file=is_valid_image_file\n",
        "        )\n",
        "        test_data = datasets.ImageFolder(\n",
        "            root=os.path.join(data_dir, 'val'),\n",
        "            transform=test_transform,\n",
        "            is_valid_file=is_valid_image_file\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR loading ImageNet-200 structure: {e}\")\n",
        "        return None, None, 200, None\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    num_classes = 200\n",
        "    print(f\"Loaded ImageNet-200: {len(train_data)} training images (with augmentation).\")\n",
        "\n",
        "    return train_loader, test_loader, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "393e82ba",
      "metadata": {
        "id": "393e82ba"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, device, epochs):\n",
        "    \"\"\"Trains the model and measures total training time.\"\"\"\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        # Use tqdm for progress bar if installed\n",
        "        for i, (inputs, labels) in enumerate(train_loader): # wrap with tqdm(train_loader) if using tqdm\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    train_time = end_time - start_time\n",
        "    return train_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c8240fa7",
      "metadata": {
        "id": "c8240fa7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_model(model, test_loader, device, img_size):\n",
        "    \"\"\"Evaluates the model, calculating accuracy and average inference time.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time_total = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass (Inference)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Accuracy calculation\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    end_time_total = time.time()\n",
        "\n",
        "\n",
        "    total_inference_time_s = end_time_total - start_time_total\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    # Calculate average time per image based on the total time and total images\n",
        "    avg_inference_time_ms = (total_inference_time_s / total) * 1000\n",
        "\n",
        "    print(f\"Total images tested: {total}\")\n",
        "    print(f\"Total inference time for dataset: {total_inference_time_s:.2f} seconds\")\n",
        "\n",
        "    return accuracy, avg_inference_time_ms\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Returns the total number of trainable parameters in the model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da4b78e",
      "metadata": {
        "id": "0da4b78e"
      },
      "source": [
        "#### **3. Experiments with different datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "IXJVHlNkLpQD",
      "metadata": {
        "id": "IXJVHlNkLpQD"
      },
      "outputs": [],
      "source": [
        "# Initialize results list\n",
        "FINAL_RESULTS = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ohr1MulwfyeC",
      "metadata": {
        "id": "ohr1MulwfyeC"
      },
      "outputs": [],
      "source": [
        "# Function to see the results\n",
        "def print_summary_table(results):\n",
        "\n",
        "    header = ['Model', 'Dataset', 'Accuracy (%)', 'Total Parameters (M)', 'Train Time (s)', 'Inference Time (ms/img)']\n",
        "    print(f\"{header[0]:<8} | {header[1]:<15} | {header[2]:<15} | {header[3]:<20} | {header[4]:<15} | {header[5]:<25}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for res in results:\n",
        "        print(f\"{res['Model']:<8} | {res['Dataset']:<15} | {res['Accuracy (%)']:<15} | {res['Total Parameters (M)']:<20} | {res['Train Time (s)']:<15} | {res['Inference Time (ms/img)']:<25}\")\n",
        "    print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fim3vbXLKZIU",
      "metadata": {
        "id": "Fim3vbXLKZIU"
      },
      "source": [
        "##### **3.1. CIFAR-10:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-cZ0Qeh9nvcW",
      "metadata": {
        "id": "-cZ0Qeh9nvcW"
      },
      "source": [
        "First, we use the Cifar-10 dataset which divide the data into 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "A7khYbZpJ9BJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7khYbZpJ9BJ",
        "outputId": "ee0f6a37-fcde-4b30-d563-2ec59e085797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "================ Starting Experiment: CIFAR-10 ================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded CIFAR-10: 50000 training images, 10000 test images (with augmentation).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running ViT on CIFAR-10 ---\n",
            "  Epoch 1/10, Loss: 1.8017\n",
            "  Epoch 2/10, Loss: 1.4386\n",
            "  Epoch 3/10, Loss: 1.3356\n",
            "  Epoch 4/10, Loss: 1.2878\n",
            "  Epoch 5/10, Loss: 1.2369\n",
            "  Epoch 6/10, Loss: 1.2012\n",
            "  Epoch 7/10, Loss: 1.1696\n",
            "  Epoch 8/10, Loss: 1.1468\n",
            "  Epoch 9/10, Loss: 1.1223\n",
            "  Epoch 10/10, Loss: 1.0961\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 3.47 seconds\n",
            "\n",
            "--- Running CNN on CIFAR-10 ---\n",
            "  Epoch 1/10, Loss: 1.4589\n",
            "  Epoch 2/10, Loss: 1.0747\n",
            "  Epoch 3/10, Loss: 0.9268\n",
            "  Epoch 4/10, Loss: 0.8212\n",
            "  Epoch 5/10, Loss: 0.7495\n",
            "  Epoch 6/10, Loss: 0.6981\n",
            "  Epoch 7/10, Loss: 0.6583\n",
            "  Epoch 8/10, Loss: 0.6230\n",
            "  Epoch 9/10, Loss: 0.5884\n",
            "  Epoch 10/10, Loss: 0.5689\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 2.86 seconds\n",
            "Model    | Dataset         | Accuracy (%)    | Total Parameters (M) | Train Time (s)  | Inference Time (ms/img)  \n",
            "----------------------------------------------------------------------------------------------------\n",
            "ViT      | CIFAR-10        | 62.92           | 1.21                 | 442.6           | 0.347                    \n",
            "CNN      | CIFAR-10        | 80.04           | 0.18                 | 323.2           | 0.286                    \n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "DATASET_NAME = 'CIFAR-10'\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# ViT Configuration for CIFAR-10 (32x32 input)\n",
        "IMG_SIZE_C10 = 32\n",
        "PATCH_SIZE = 4 # (32/4)^2 + 1 = 65 tokens\n",
        "\n",
        "print(f\"\\n\\n================ Starting Experiment: {DATASET_NAME} ================\")\n",
        "\n",
        "# 1. Load Data\n",
        "train_loader, test_loader, num_classes = get_cifar_loaders(DATASET_NAME, BATCH_SIZE)\n",
        "\n",
        "# 2. Instantiate Models\n",
        "vit_model = VisionTransformer(\n",
        "    img_size=IMG_SIZE_C10, patch_size=PATCH_SIZE, num_classes=num_classes,\n",
        "    embed_dim=128, depth=6, num_heads=8\n",
        ").to(DEVICE)\n",
        "\n",
        "cnn_model = CustomCNN(num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "# --- ViT Run ---\n",
        "print(f\"\\n--- Running ViT on {DATASET_NAME} ---\")\n",
        "optimizer_vit = optim.Adam(vit_model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_time_vit = train_model(vit_model, train_loader, criterion, optimizer_vit, DEVICE, NUM_EPOCHS)\n",
        "accuracy_vit, inference_time_vit = test_model(vit_model, test_loader, DEVICE,IMG_SIZE_C10)\n",
        "params_vit = count_parameters(vit_model)\n",
        "\n",
        "FINAL_RESULTS.append({\n",
        "    'Model': 'ViT', 'Dataset': DATASET_NAME,\n",
        "    'Accuracy (%)': f\"{accuracy_vit:.2f}\",\n",
        "    'Train Time (s)': f\"{train_time_vit:.1f}\",\n",
        "    'Inference Time (ms/img)': f\"{inference_time_vit:.3f}\",\n",
        "    'Total Parameters (M)': f\"{params_vit/1e6:.2f}\"\n",
        "})\n",
        "\n",
        "# --- CNN Run ---\n",
        "print(f\"\\n--- Running CNN on {DATASET_NAME} ---\")\n",
        "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
        "train_time_cnn = train_model(cnn_model, train_loader, criterion, optimizer_cnn, DEVICE, NUM_EPOCHS)\n",
        "accuracy_cnn, inference_time_cnn = test_model(cnn_model, test_loader, DEVICE, IMG_SIZE_C10)\n",
        "params_cnn = count_parameters(cnn_model)\n",
        "\n",
        "FINAL_RESULTS.append({\n",
        "    'Model': 'CNN', 'Dataset': DATASET_NAME,\n",
        "    'Accuracy (%)': f\"{accuracy_cnn:.2f}\",\n",
        "    'Train Time (s)': f\"{train_time_cnn:.1f}\",\n",
        "    'Inference Time (ms/img)': f\"{inference_time_cnn:.3f}\",\n",
        "    'Total Parameters (M)': f\"{params_cnn/1e6:.2f}\"\n",
        "})\n",
        "\n",
        "print_summary_table(FINAL_RESULTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OLXgi-2noJRm",
      "metadata": {
        "id": "OLXgi-2noJRm"
      },
      "source": [
        "We can observe clearly that for the Cifar-10 dataset, the CNN is better than ViT in both accuracy, training time and inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "KVIgUp0_rDc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVIgUp0_rDc9",
        "outputId": "122f3a9c-caa5-472c-c523-24671fe3b9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionTransformer(\n",
            "  (patch_embed): PatchEmbedding(\n",
            "    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.1, inplace=False)\n",
            "  (blocks): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
            "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(vit_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "K8biGtLDrGNc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8biGtLDrGNc",
        "outputId": "5a54541d-2e5a-4c88-8c22-a1c7ee168c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomCNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "  )\n",
            "  (layer1): Sequential(\n",
            "    (0): CNNBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): CNNBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): CNNBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): CNNBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): CNNBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): CNNBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(cnn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LtyTgOkFKmWx",
      "metadata": {
        "id": "LtyTgOkFKmWx"
      },
      "source": [
        "##### **3.2. CIFAR-100**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sj3yxmyeoZkn",
      "metadata": {
        "id": "Sj3yxmyeoZkn"
      },
      "source": [
        "Next, we will test our architectures on the Cifar-100 datasets, which includes labelled data of 100 different classes. This dataset have much more classes than the Cifar-10 but it has the same number of training images (50000) and test images (10000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "xaouPrRDJ_UY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaouPrRDJ_UY",
        "outputId": "c784fd38-7fee-457d-d4cc-f6f1ad9b3368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "================ Starting Experiment: CIFAR-100 ================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:19<00:00, 8.71MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded CIFAR-100: 50000 training images, 10000 test images (with augmentation).\n",
            "\n",
            "--- Running ViT on CIFAR-100 ---\n",
            "  Epoch 1/10, Loss: 3.9135\n",
            "  Epoch 2/10, Loss: 3.3593\n",
            "  Epoch 3/10, Loss: 3.1276\n",
            "  Epoch 4/10, Loss: 2.9644\n",
            "  Epoch 5/10, Loss: 2.8447\n",
            "  Epoch 6/10, Loss: 2.7469\n",
            "  Epoch 7/10, Loss: 2.6607\n",
            "  Epoch 8/10, Loss: 2.5911\n",
            "  Epoch 9/10, Loss: 2.5183\n",
            "  Epoch 10/10, Loss: 2.4560\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 3.49 seconds\n",
            "\n",
            "--- Running CNN on CIFAR-100 ---\n",
            "  Epoch 1/10, Loss: 3.9000\n",
            "  Epoch 2/10, Loss: 3.3384\n",
            "  Epoch 3/10, Loss: 2.9782\n",
            "  Epoch 4/10, Loss: 2.7417\n",
            "  Epoch 5/10, Loss: 2.5488\n",
            "  Epoch 6/10, Loss: 2.4081\n",
            "  Epoch 7/10, Loss: 2.2899\n",
            "  Epoch 8/10, Loss: 2.1946\n",
            "  Epoch 9/10, Loss: 2.1052\n",
            "  Epoch 10/10, Loss: 2.0448\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 2.46 seconds\n",
            "Model    | Dataset         | Accuracy (%)    | Total Parameters (M) | Train Time (s)  | Inference Time (ms/img)  \n",
            "----------------------------------------------------------------------------------------------------\n",
            "ViT      | CIFAR-10        | 62.92           | 1.21                 | 442.6           | 0.347                    \n",
            "CNN      | CIFAR-10        | 80.04           | 0.18                 | 323.2           | 0.286                    \n",
            "ViT      | CIFAR-100       | 36.77           | 1.22                 | 438.8           | 0.349                    \n",
            "CNN      | CIFAR-100       | 42.87           | 0.18                 | 326.3           | 0.246                    \n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "DATASET_NAME = 'CIFAR-100'\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# ViT Configuration for CIFAR-10 (32x32 input)\n",
        "IMG_SIZE_C100 = 32\n",
        "PATCH_SIZE = 4 # (32/4)^2 + 1 = 65 tokens\n",
        "\n",
        "print(f\"\\n\\n================ Starting Experiment: {DATASET_NAME} ================\")\n",
        "\n",
        "# 1. Load Data\n",
        "train_loader, test_loader, num_classes = get_cifar_loaders(DATASET_NAME, BATCH_SIZE)\n",
        "\n",
        "# 2. Instantiate Models\n",
        "vit_model = VisionTransformer(\n",
        "    img_size=IMG_SIZE_C100, patch_size=PATCH_SIZE, num_classes=num_classes,\n",
        "    embed_dim=128, depth=6, num_heads=8\n",
        ").to(DEVICE)\n",
        "\n",
        "cnn_model = CustomCNN(num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "# --- ViT Run ---\n",
        "print(f\"\\n--- Running ViT on {DATASET_NAME} ---\")\n",
        "optimizer_vit = optim.Adam(vit_model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train_time_vit = train_model(vit_model, train_loader, criterion, optimizer_vit, DEVICE, NUM_EPOCHS)\n",
        "accuracy_vit, inference_time_vit = test_model(vit_model, test_loader, DEVICE, IMG_SIZE_C100)\n",
        "params_vit = count_parameters(vit_model)\n",
        "\n",
        "FINAL_RESULTS.append({\n",
        "    'Model': 'ViT', 'Dataset': DATASET_NAME,\n",
        "    'Accuracy (%)': f\"{accuracy_vit:.2f}\",\n",
        "    'Train Time (s)': f\"{train_time_vit:.1f}\",\n",
        "    'Inference Time (ms/img)': f\"{inference_time_vit:.3f}\",\n",
        "    'Total Parameters (M)': f\"{params_vit/1e6:.2f}\"\n",
        "})\n",
        "\n",
        "# --- CNN Run ---\n",
        "print(f\"\\n--- Running CNN on {DATASET_NAME} ---\")\n",
        "optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
        "train_time_cnn = train_model(cnn_model, train_loader, criterion, optimizer_cnn, DEVICE, NUM_EPOCHS)\n",
        "accuracy_cnn, inference_time_cnn = test_model(cnn_model, test_loader, DEVICE, IMG_SIZE_C100)\n",
        "params_cnn = count_parameters(cnn_model)\n",
        "\n",
        "FINAL_RESULTS.append({\n",
        "    'Model': 'CNN', 'Dataset': DATASET_NAME,\n",
        "    'Accuracy (%)': f\"{accuracy_cnn:.2f}\",\n",
        "    'Train Time (s)': f\"{train_time_cnn:.1f}\",\n",
        "    'Inference Time (ms/img)': f\"{inference_time_cnn:.3f}\",\n",
        "    'Total Parameters (M)': f\"{params_cnn/1e6:.2f}\"\n",
        "})\n",
        "print_summary_table(FINAL_RESULTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L7mw_rL_ovPP",
      "metadata": {
        "id": "L7mw_rL_ovPP"
      },
      "source": [
        "For this dataset, the result is still the same with the Cifar-10. Although the accuracy is lower than the Cifar-10 dataset (because we have more classes and we just train on 10 epoches and the architecture are not complex enough), the CNN is still better in accuracy, training time and inference time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2lHyveKyKvFp",
      "metadata": {
        "id": "2lHyveKyKvFp"
      },
      "source": [
        "##### **3.3. Image-net 200:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ypW4pgiB6yeh",
      "metadata": {
        "id": "ypW4pgiB6yeh"
      },
      "source": [
        "Lastly, we use the Image-net 200 dataset. This is a small version of Image-net dataset, which has 200 classes. This dataset have 42905 training images and 10000 test images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = 'ImageNet-200'\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# ViT Configuration for ImageNet-200 (64x64 input)\n",
        "IMG_SIZE_200 = 64\n",
        "PATCH_SIZE = 8 # (64/8)^2 + 1 = 65 tokens\n",
        "\n",
        "print(f\"\\n\\n================ Starting Experiment: {DATASET_NAME} ================\")\n",
        "\n",
        "#  Download and Prepare Data\n",
        "TINY_IMAGENET_ROOT = None\n",
        "try:\n",
        "    TINY_IMAGENET_PATH = kagglehub.dataset_download(\"akash2sharma/tiny-imagenet\")\n",
        "    TINY_IMAGENET_ROOT = os.path.join(TINY_IMAGENET_PATH, 'tiny-imagenet-200')\n",
        "    if not os.path.isdir(TINY_IMAGENET_ROOT):\n",
        "        TINY_IMAGENET_ROOT = TINY_IMAGENET_PATH\n",
        "except Exception as e:\n",
        "    print(f\"KaggleHub download failed. Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq1rohubJn4v",
        "outputId": "33295b13-359b-4dab-d954-cedbc896693c"
      },
      "id": "Yq1rohubJn4v",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "================ Starting Experiment: ImageNet-200 ================\n",
            "Using Colab cache for faster access to the 'tiny-imagenet' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "93f1bdb6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93f1bdb6",
        "outputId": "86e19ec4-ddc2-4e9b-eb59-014bde60469c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying data from /kaggle/input/tiny-imagenet/tiny-imagenet-200 to /kaggle/working/tiny-imagenet-writable...\n",
            "Data copied successfully.\n",
            "Sorting Tiny ImageNet validation set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing validation images: 100%|██████████| 10000/10000 [00:00<00:00, 29616.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set sorting complete.\n",
            "Loaded ImageNet-200: 100000 training images (with augmentation).\n",
            "\n",
            "--- Running ViT on ImageNet-200 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Epoch 1/10, Loss: 4.6694\n",
            "  Epoch 2/10, Loss: 4.0706\n",
            "  Epoch 3/10, Loss: 3.7755\n",
            "  Epoch 4/10, Loss: 3.5902\n",
            "  Epoch 5/10, Loss: 3.4520\n",
            "  Epoch 6/10, Loss: 3.3436\n",
            "  Epoch 7/10, Loss: 3.2556\n",
            "  Epoch 8/10, Loss: 3.1794\n",
            "  Epoch 9/10, Loss: 3.1173\n",
            "  Epoch 10/10, Loss: 3.0640\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 5.55 seconds\n",
            "\n",
            "--- Running CNN on ImageNet-200 ---\n",
            "  Epoch 1/10, Loss: 4.6212\n",
            "  Epoch 2/10, Loss: 4.0275\n",
            "  Epoch 3/10, Loss: 3.7576\n",
            "  Epoch 4/10, Loss: 3.5650\n",
            "  Epoch 5/10, Loss: 3.4166\n",
            "  Epoch 6/10, Loss: 3.3028\n",
            "  Epoch 7/10, Loss: 3.2052\n",
            "  Epoch 8/10, Loss: 3.1293\n",
            "  Epoch 9/10, Loss: 3.0652\n",
            "  Epoch 10/10, Loss: 3.0032\n",
            "Total images tested: 10000\n",
            "Total inference time for dataset: 4.92 seconds\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "WRITABLE_DATA_ROOT = None\n",
        "if TINY_IMAGENET_ROOT:\n",
        "    WRITABLE_DATA_ROOT = prepare_writable_data(TINY_IMAGENET_ROOT)\n",
        "\n",
        "if WRITABLE_DATA_ROOT is None:\n",
        "    print(\"Skipping ImageNet-200 experiment due to data preparation failure.\")\n",
        "else:\n",
        "    # 2. Load Data (Sorting happens inside this call)\n",
        "    train_loader, test_loader, num_classes = get_imagenet_200_loaders(WRITABLE_DATA_ROOT, BATCH_SIZE)\n",
        "\n",
        "    if train_loader is None:\n",
        "\n",
        "        print(\"Data loading failed.\")\n",
        "    else:\n",
        "        # 3. Instantiate Models\n",
        "        vit_model = VisionTransformer(\n",
        "            img_size=IMG_SIZE_200, patch_size=PATCH_SIZE, num_classes=num_classes,\n",
        "            embed_dim=128, depth=6, num_heads=8\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        cnn_model = CustomCNN(num_classes=num_classes).to(DEVICE)\n",
        "\n",
        "        # --- ViT Run ---\n",
        "        print(f\"\\n--- Running ViT on {DATASET_NAME} ---\")\n",
        "        optimizer_vit = optim.Adam(vit_model.parameters(), lr=LEARNING_RATE)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        train_time_vit = train_model(vit_model, train_loader, criterion, optimizer_vit, DEVICE, NUM_EPOCHS)\n",
        "        accuracy_vit, inference_time_vit = test_model(vit_model, test_loader, DEVICE, IMG_SIZE_200)\n",
        "        params_vit = count_parameters(vit_model)\n",
        "\n",
        "        FINAL_RESULTS.append({\n",
        "            'Model': 'ViT', 'Dataset': DATASET_NAME,\n",
        "            'Accuracy (%)': f\"{accuracy_vit:.2f}\",\n",
        "            'Train Time (s)': f\"{train_time_vit:.1f}\",\n",
        "            'Inference Time (ms/img)': f\"{inference_time_vit:.3f}\",\n",
        "            'Total Parameters (M)': f\"{params_vit/1e6:.2f}\"\n",
        "        })\n",
        "\n",
        "        # --- CNN Run ---\n",
        "        print(f\"\\n--- Running CNN on {DATASET_NAME} ---\")\n",
        "        optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
        "        train_time_cnn = train_model(cnn_model, train_loader, criterion, optimizer_cnn, DEVICE, NUM_EPOCHS)\n",
        "        accuracy_cnn, inference_time_cnn = test_model(cnn_model, test_loader, DEVICE, IMG_SIZE_200)\n",
        "        params_cnn = count_parameters(cnn_model)\n",
        "\n",
        "        FINAL_RESULTS.append({\n",
        "            'Model': 'CNN', 'Dataset': DATASET_NAME,\n",
        "            'Accuracy (%)': f\"{accuracy_cnn:.2f}\",\n",
        "            'Train Time (s)': f\"{train_time_cnn:.1f}\",\n",
        "            'Inference Time (ms/img)': f\"{inference_time_cnn:.3f}\",\n",
        "            'Total Parameters (M)': f\"{params_cnn/1e6:.2f}\"\n",
        "        })\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "DMx9CsGBMWEp",
      "metadata": {
        "id": "DMx9CsGBMWEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79588322-84a7-4664-94f3-aafa659333cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model    | Dataset         | Accuracy (%)    | Total Parameters (M) | Train Time (s)  | Inference Time (ms/img)  \n",
            "----------------------------------------------------------------------------------------------------\n",
            "ViT      | CIFAR-10        | 62.92           | 1.21                 | 442.6           | 0.347                    \n",
            "CNN      | CIFAR-10        | 80.04           | 0.18                 | 323.2           | 0.286                    \n",
            "ViT      | CIFAR-100       | 36.77           | 1.22                 | 438.8           | 0.349                    \n",
            "CNN      | CIFAR-100       | 42.87           | 0.18                 | 326.3           | 0.246                    \n",
            "ViT      | ImageNet-200    | 31.15           | 1.25                 | 883.9           | 0.555                    \n",
            "CNN      | ImageNet-200    | 29.85           | 0.19                 | 838.6           | 0.492                    \n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print_summary_table(FINAL_RESULTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CSZ7rJX__y7q",
      "metadata": {
        "id": "CSZ7rJX__y7q"
      },
      "source": [
        "For ImageNet-200 dataset, we can clearly observe that the ViT outperforms the CNN in terms of accuracy, while the training time and inference time do not show any significant differences (less than a one-minute difference). This result is consistent with the theory that, for datasets with a large number of classes, ViTs tend to outperform CNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78850bf",
      "metadata": {
        "id": "b78850bf"
      },
      "source": [
        "### **Summary of CNNs and ViTs**\n",
        "\n",
        "| Features | CNNs | ViTs |\n",
        "| :--- | :--- | :--- |\n",
        "| **Attention Scope** | Capture local features via convolutions | Capture global relationships via self-attention |\n",
        "| **Inductive Bias** | Strong biases (locality, translation invariance) | Minimal biases, more flexible but data-hungry |\n",
        "| **Data Requirement** | Work well with small datasets | Need large datasets for best performance |\n",
        "| **Feature Learning** | Learn hierarchical features | Learn context-rich, long-range features |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56c69f27",
      "metadata": {
        "id": "56c69f27"
      },
      "source": [
        "### **Comparative Analysis and Conclusion**\n",
        "\n",
        "#### **Performance on Small Datasets (e.g., CIFAR-10, CIFAR-100)**\n",
        "\n",
        "For datasets characterized by limited data volume and a small number of classes, the Convolutional Neural Network (CNN) architecture demonstrates superior performance across multiple metrics compared to the Vision Transformer (ViT).\n",
        "\n",
        "*   **Performance:** CNNs consistently achieve higher classification accuracy.\n",
        "*   **Efficiency:** CNNs exhibit significantly shorter training times and lower inference latency.\n",
        "*   **Cost:** The CNN architecture requires a substantially smaller number of trainable parameters, resulting in a lower memory footprint and computational cost.\n",
        "\n",
        "This outcome is attributed to the strong **inductive biases** (locality and translation invariance) inherent in CNNs, which enable effective feature learning from limited data.\n",
        "\n",
        "#### **Performance on Large-Scale Datasets (e.g., ImageNet-200)**\n",
        "\n",
        "When transitioning to large datasets featuring high data volume and a large number of classes, the comparative advantage shifts:\n",
        "\n",
        "*   **Accuracy:** ViT architectures generally achieve superior classification accuracy, demonstrating their ability to leverage extensive data to learn complex, global feature representations without relying on local biases.\n",
        "*   **Efficiency:** While ViTs are often computationally slower than optimized CNNs, the difference in training and inference time becomes less pronounced or even slightly favors the ViT in certain highly optimized implementations, particularly when considering the superior accuracy achieved.\n",
        "*   **Cost and Resource Allocation:** A major drawback of the ViT is its **high parameter count**. ViTs typically require a significantly larger number of parameters than comparable CNNs (e.g., ResNet variants) to achieve peak performance. This translates directly to higher memory requirements and greater computational expense during both training and deployment.\n",
        "\n",
        "#### **Conclusion on Architectural Choice**\n",
        "\n",
        "The choice between a CNN and a ViT should be dictated by the available resources and the project's primary objective:\n",
        "\n",
        "1.  **Resource-Constrained Environments:** If computational budget, memory constraints, or training time are critical factors, the **CNN architecture (e.g., ResNet)** remains the preferred and most resource-efficient choice, often providing acceptable accuracy with minimal cost.\n",
        "2.  **Accuracy-Driven Objectives:** If the primary goal is maximizing classification accuracy and resources are abundant, the **Vision Transformer** is the superior choice, as its global attention mechanism allows it to achieve state-of-the-art results by effectively modeling long-range dependencies in large datasets. The higher computational cost is accepted as a necessary trade-off for improved performance.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}